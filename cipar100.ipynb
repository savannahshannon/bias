{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWwpNzP6PFVWji4G0EAuSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savannahshannon/bias/blob/main/cipar100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIPAR-100"
      ],
      "metadata": {
        "id": "_Ce3CB_zwdUM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ4MSUZzwZsa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Model Hyperparameters\n",
        "LATENT_DIM = 128\n",
        "EPOCHS = 150 # Reduced for quicker execution, can be increased\n",
        "CLASSIFIER_EPOCHS = 50 # Reduced for quicker execution, can be increased\n",
        "BATCH_SIZE = 64\n",
        "IMG_SHAPE = (32, 32, 3)\n",
        "NUM_CLASSES = 100 # Changed for CIFAR-100\n",
        "\n",
        "# Define the class names for CIFAR-100 for better visualization\n",
        "# Source: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "CIFAR100_CLASS_NAMES = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'garden_shrew', 'girl',\n",
        "    'globe', 'goat', 'goldfish', 'gorilla', 'grain', 'grass', 'hamster', 'house',\n",
        "    'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard',\n",
        "    'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom',\n",
        "    'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck',\n",
        "    'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit',\n",
        "    'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew',\n",
        "    'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'stock_car',\n",
        "    'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television',\n",
        "    'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale',\n",
        "    'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "\n",
        "def plot_class_distribution(labels, title, class_names):\n",
        "    \"\"\"Helper function to plot the distribution of classes.\"\"\"\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    class_map = {i: 0 for i in range(len(class_names))}\n",
        "    for i, cls in enumerate(unique):\n",
        "        class_map[cls] = counts[i]\n",
        "\n",
        "    sorted_counts = [class_map[i] for i in range(len(class_names))]\n",
        "\n",
        "    plt.figure(figsize=(18, 6)) # Increased figure size for 100 classes\n",
        "    sns.barplot(x=class_names, y=sorted_counts)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.xticks(rotation=90, fontsize=6) # Rotate and shrink labels\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "\n",
        "# --- Step 1: Load and Preprocess Data ---\n",
        "print(\"Step 1: Loading and preprocessing the CIFAR-100 dataset...\")\n",
        "(x_train_orig, y_train_orig), (x_test, y_test) = tf.keras.datasets.cifar100.load_data() # Changed to cifar100\n",
        "y_train_orig = y_train_orig.flatten()\n",
        "y_test = y_test.flatten()\n",
        "\n",
        "# GANs work best with data normalized to the [-1, 1] range\n",
        "x_train_gan_norm = (x_train_orig.astype('float32') - 127.5) / 127.5\n",
        "print(f\"Original training data shape: {x_train_gan_norm.shape}\")\n",
        "print(f\"Test data shape: {x_test.shape}\")\n",
        "\n",
        "# --- Step 2: Artificially Create an Imbalanced Dataset ---\n",
        "print(\"\\nStep 2: Artificially creating an imbalanced dataset...\")\n",
        "# Keep fewer samples for higher class indices - adjusted for 100 classes\n",
        "# Using a slower decay to ensure some classes remain for all 100\n",
        "keep_percentages = {i: (1.0 - i * 0.009) for i in range(NUM_CLASSES)}\n",
        "# Ensure no class has less than 10% of its original samples\n",
        "for i in range(NUM_CLASSES):\n",
        "    if keep_percentages[i] < 0.1:\n",
        "        keep_percentages[i] = 0.1\n",
        "\n",
        "x_train_imbalanced_list = []\n",
        "y_train_imbalanced_list = []\n",
        "\n",
        "for class_id in range(NUM_CLASSES):\n",
        "    class_indices = np.where(y_train_orig == class_id)[0]\n",
        "    num_to_keep = int(len(class_indices) * keep_percentages[class_id])\n",
        "    keep_indices = np.random.choice(class_indices, size=num_to_keep, replace=False)\n",
        "    x_train_imbalanced_list.append(x_train_gan_norm[keep_indices])\n",
        "    y_train_imbalanced_list.append(y_train_orig[keep_indices])\n",
        "\n",
        "x_train_imbalanced_gan_norm = np.concatenate(x_train_imbalanced_list, axis=0)\n",
        "y_train_imbalanced = np.concatenate(y_train_imbalanced_list, axis=0)\n",
        "\n",
        "print(f\"Shape of imbalanced training data: {x_train_imbalanced_gan_norm.shape}\")\n",
        "plot_class_distribution(y_train_imbalanced, 'Artificially Imbalanced CIFAR-100 Distribution', CIFAR100_CLASS_NAMES)\n",
        "\n",
        "\n",
        "# --- Step 3: Define the GAN Components ---\n",
        "print(\"\\nStep 3: Building the Generator and Discriminator models...\")\n",
        "\n",
        "# -- Generator --\n",
        "def build_generator():\n",
        "    latent_input = layers.Input(shape=(LATENT_DIM,))\n",
        "    label_input = layers.Input(shape=(1,))\n",
        "\n",
        "    label_embedding = layers.Embedding(NUM_CLASSES, 100)(label_input) # Embedding dimension increased for 100 classes\n",
        "    # Merge label and latent vector\n",
        "    merged_input = layers.Concatenate()([latent_input, layers.Flatten()(label_embedding)])\n",
        "\n",
        "    # Project and reshape\n",
        "    model = layers.Dense(4 * 4 * 256, use_bias=False)(merged_input)\n",
        "    model = layers.BatchNormalization()(model)\n",
        "    model = layers.LeakyReLU(alpha=0.2)(model)\n",
        "    model = layers.Reshape((4, 4, 256))(model)\n",
        "\n",
        "    # Upsampling block 1: 4x4 -> 8x8\n",
        "    model = layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=False)(model)\n",
        "    model = layers.BatchNormalization()(model)\n",
        "    model = layers.LeakyReLU(alpha=0.2)(model)\n",
        "\n",
        "    # Upsampling block 2: 8x8 -> 16x16\n",
        "    model = layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False)(model)\n",
        "    model = layers.BatchNormalization()(model)\n",
        "    model = layers.LeakyReLU(alpha=0.2)(model)\n",
        "\n",
        "    # Upsampling block 3: 16x16 -> 32x32\n",
        "    # Final layer uses tanh activation to scale output to [-1, 1]\n",
        "    output_image = layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(model)\n",
        "\n",
        "    return tf.keras.Model([latent_input, label_input], output_image)\n",
        "\n",
        "# -- Discriminator --\n",
        "def build_discriminator():\n",
        "    image_input = layers.Input(shape=IMG_SHAPE)\n",
        "    label_input = layers.Input(shape=(1,))\n",
        "\n",
        "    label_embedding = layers.Embedding(NUM_CLASSES, 100)(label_input) # Embedding dimension increased for 100 classes\n",
        "    label_embedding = layers.Dense(IMG_SHAPE[0] * IMG_SHAPE[1] * 1)(label_embedding) # Adjust dense layer output size\n",
        "    label_embedding = layers.Reshape((IMG_SHAPE[0], IMG_SHAPE[1], 1))(label_embedding)\n",
        "\n",
        "    merged_input = layers.Concatenate()([image_input, label_embedding])\n",
        "\n",
        "    # Downsampling block 1\n",
        "    model = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(merged_input)\n",
        "    model = layers.LeakyReLU(alpha=0.2)(model)\n",
        "    model = layers.Dropout(0.4)(model) # Increased dropout\n",
        "\n",
        "    # Downsampling block 2\n",
        "    model = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(model)\n",
        "    model = layers.BatchNormalization()(model)\n",
        "    model = layers.LeakyReLU(alpha=0.2)(model)\n",
        "    model = layers.Dropout(0.4)(model) # Increased dropout\n",
        "\n",
        "    # Downsampling block 3\n",
        "    model = layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(model)\n",
        "    model = layers.BatchNormalization()(model)\n",
        "    model = layers.LeakyReLU(alpha=0.2)(model)\n",
        "    model = layers.Dropout(0.4)(model) # Increased dropout\n",
        "\n",
        "    model = layers.Flatten()(model)\n",
        "    validity = layers.Dense(1, activation='sigmoid')(model)\n",
        "    return tf.keras.Model([image_input, label_input], validity)\n",
        "\n",
        "# Create the models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "#print(generator.summary()) # comment out to get rid of the summary table\n",
        "#print(discriminator.summary())\n",
        "\n",
        "# --- Step 4: Define Loss, Optimizers, and Training Step ---\n",
        "print(\"\\nStep 4: Defining loss, optimizers, and the training step function...\")\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "# The core training logic for one batch\n",
        "@tf.function\n",
        "def train_step(real_images, real_labels):\n",
        "    noise = tf.random.normal([real_images.shape[0], LATENT_DIM])\n",
        "\n",
        "    real_labels_smooth = tf.ones_like(discriminator([real_images, real_labels], training=False)) * 0.9\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator([noise, real_labels], training=True)\n",
        "\n",
        "        real_output = discriminator([real_images, real_labels], training=True)\n",
        "        fake_output = discriminator([generated_images, real_labels], training=True)\n",
        "\n",
        "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "        real_loss = cross_entropy(real_labels_smooth, real_output)\n",
        "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "        disc_loss = real_loss + fake_loss\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "# --- Step 5: Train the cGAN ---\n",
        "print(f\"\\nStep 5: Training the cGAN for {EPOCHS} epochs...\")\n",
        "\n",
        "# Create a tf.data.Dataset for efficient training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_imbalanced_gan_norm, y_train_imbalanced)).shuffle(len(x_train_imbalanced_gan_norm)).batch(BATCH_SIZE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_gen_loss = 0\n",
        "    total_disc_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for image_batch, label_batch in train_dataset:\n",
        "        gen_loss, disc_loss = train_step(image_batch, label_batch)\n",
        "        total_gen_loss += gen_loss\n",
        "        total_disc_loss += disc_loss\n",
        "        num_batches +=1\n",
        "\n",
        "    avg_gen_loss = total_gen_loss / num_batches\n",
        "    avg_disc_loss = total_disc_loss / num_batches\n",
        "\n",
        "    print (f'Time for epoch {epoch + 1} is {time.time()-start:.2f} sec. Gen Loss: {avg_gen_loss:.4f}, Disc Loss: {avg_disc_loss:.4f}')\n",
        "\n",
        "print(\"cGAN Training complete.\")\n",
        "# --- Step 6: Generate Synthetic Data to Balance the Dataset ---\n",
        "print(\"\\nStep 6: Generating synthetic data to re-balance the dataset...\")\n",
        "\n",
        "class_counts = np.bincount(y_train_imbalanced, minlength=NUM_CLASSES)\n",
        "majority_count = np.max(class_counts)\n",
        "print(f\"Majority class has {majority_count} samples. Balancing all classes to this count.\")\n",
        "\n",
        "# Denormalize the original imbalanced set to add it back\n",
        "x_balanced_list = [(x_train_imbalanced_gan_norm * 127.5 + 127.5).astype(np.uint8)]\n",
        "y_balanced_list = [y_train_imbalanced]\n",
        "\n",
        "for class_id in range(NUM_CLASSES):\n",
        "    current_count = class_counts[class_id]\n",
        "    num_to_generate = majority_count - current_count\n",
        "\n",
        "    if num_to_generate > 0:\n",
        "        print(f\"Generating {num_to_generate} new images for class '{CIFAR100_CLASS_NAMES[class_id]}' (Class ID: {class_id})\")\n",
        "\n",
        "        # Prepare inputs for the generator\n",
        "        noise = tf.random.normal([num_to_generate, LATENT_DIM])\n",
        "        labels_to_generate = np.full(num_to_generate, class_id)\n",
        "\n",
        "        # Convert labels_to_generate to a TensorFlow tensor\n",
        "        labels_to_generate_tensor = tf.constant(labels_to_generate, dtype=tf.int32)\n",
        "\n",
        "        # Generate images\n",
        "        synthetic_images = generator([noise, labels_to_generate_tensor], training=False) # Pass the tensor\n",
        "\n",
        "        # Denormalize images from [-1, 1] back to [0, 255]\n",
        "        synthetic_images_denorm = (synthetic_images.numpy() * 127.5 + 127.5).astype(np.uint8)\n",
        "\n",
        "        # Append to our balanced list\n",
        "        x_balanced_list.append(synthetic_images_denorm)\n",
        "        y_balanced_list.append(labels_to_generate)\n",
        "\n",
        "# Create the final balanced dataset\n",
        "x_train_balanced = np.concatenate(x_balanced_list, axis=0)\n",
        "y_train_balanced = np.concatenate(y_balanced_list, axis=0)\n",
        "\n",
        "print(f\"\\nShape of final balanced training data: {x_train_balanced.shape}\")\n",
        "\n",
        "\n",
        "# --- Step 7: Verify the Final Balanced Dataset ---\n",
        "print(\"\\nStep 7: Verifying the distribution of the final, balanced dataset...\")\n",
        "plot_class_distribution(y_train_balanced, 'Final Re-balanced CIFAR-100 Distribution (using cGAN)', CIFAR100_CLASS_NAMES)\n",
        "\n",
        "\n",
        "# --- Step 8: Visualize Sample Generated Images ---\n",
        "print(\"\\nStep 8: Displaying some sample generated images...\")\n",
        "\n",
        "def show_generated_images(rows=10, cols=10): # Changed to 10x10 for 100 classes\n",
        "    noise = tf.random.normal([rows * cols, LATENT_DIM])\n",
        "    # Generate one image of each class for the first 100 spots\n",
        "    sampled_labels = np.array([i for i in range(NUM_CLASSES)])\n",
        "\n",
        "    # Convert sampled_labels to a TensorFlow tensor\n",
        "    sampled_labels_tensor = tf.constant(sampled_labels, dtype=tf.int32)\n",
        "\n",
        "    generated_images = generator.predict([noise, sampled_labels_tensor]) # Pass the tensor\n",
        "    generated_images = 0.5 * generated_images + 0.5 # Rescale to [0, 1] for plotting\n",
        "\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(20,20)) # Increased figure size\n",
        "    cnt = 0\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            if cnt < NUM_CLASSES: # Only plot up to NUM_CLASSES\n",
        "                axs[i,j].imshow(generated_images[cnt, :,:,:])\n",
        "                axs[i,j].set_title(f\"Gen: {CIFAR100_CLASS_NAMES[sampled_labels[cnt]]}\", fontsize=8) # Smaller font for title\n",
        "                axs[i,j].axis('off')\n",
        "            else:\n",
        "                axs[i,j].axis('off') # Hide extra subplots if NUM_CLASSES < rows*cols\n",
        "            cnt += 1\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_generated_images()\n",
        "\n",
        "# --- Step 9: Define the Classifier Model ---\n",
        "print(\"\\nStep 9: Defining a standard CNN classifier for evaluation...\")\n",
        "def build_classifier():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Input(shape=IMG_SHAPE),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'), layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'), layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'), layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(), layers.Dense(256, activation='relu'), layers.Dropout(0.5), # Increased Dense layer size\n",
        "        layers.Dense(NUM_CLASSES, activation='softmax') # Changed for 100 classes\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, class_names, title):\n",
        "    \"\"\"Helper function to plot a confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(25, 25)) # Increased figure size for 100 classes\n",
        "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', # Turned off annot for readability with 100 classes\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.xticks(rotation=90, fontsize=4)\n",
        "    plt.yticks(rotation=0, fontsize=4)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Prepare data for classifier (normalize to [0, 1])\n",
        "# We need to denormalize the [-1,1] imbalanced data first, then re-normalize to [0,1]\n",
        "x_train_imbalanced_uint8 = ((x_train_imbalanced_gan_norm * 127.5) + 127.5).astype(np.uint8)\n",
        "x_train_imbalanced_clf_norm = x_train_imbalanced_uint8 / 255.0\n",
        "x_train_balanced_clf_norm = x_train_balanced / 255.0\n",
        "x_test_clf_norm = x_test / 255.0\n",
        "\n",
        "# --- Step 10: Train and Evaluate the BASELINE Model (on imbalanced data) ---\n",
        "print(f\"\\nStep 10: Training BASELINE classifier for {CLASSIFIER_EPOCHS} epochs on Imbalanced data...\")\n",
        "baseline_classifier = build_classifier()\n",
        "baseline_classifier.fit(\n",
        "    x_train_imbalanced_clf_norm, y_train_imbalanced,\n",
        "    epochs=CLASSIFIER_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x_test_clf_norm, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "print(\"\\n--- BASELINE MODEL EVALUATION ---\")\n",
        "y_pred_baseline = baseline_classifier.predict(x_test_clf_norm)\n",
        "y_pred_classes_baseline = np.argmax(y_pred_baseline, axis=1)\n",
        "print(classification_report(y_test, y_pred_classes_baseline, target_names=CIFAR100_CLASS_NAMES))\n",
        "plot_confusion_matrix(y_test, y_pred_classes_baseline, CIFAR100_CLASS_NAMES, 'Baseline Model Confusion Matrix')\n",
        "\n",
        "\n",
        "# --- Step 11: Train and Evaluate the FINAL Model (on balanced data) ---\n",
        "print(f\"\\nStep 11: Training FINAL classifier for {CLASSIFIER_EPOCHS} epochs on GAN-BALANCED data...\")\n",
        "final_classifier = build_classifier()\n",
        "final_classifier.fit(\n",
        "    x_train_balanced_clf_norm, y_train_balanced,\n",
        "    epochs=CLASSIFIER_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(x_test_clf_norm, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "print(\"\\n--- FINAL MODEL EVALUATION ---\")\n",
        "y_pred_final = final_classifier.predict(x_test_clf_norm)\n",
        "y_pred_classes_final = np.argmax(y_pred_final, axis=1)\n",
        "print(classification_report(y_test, y_pred_classes_final, target_names=CIFAR100_CLASS_NAMES))\n",
        "plot_confusion_matrix(y_test, y_pred_classes_final, CIFAR100_CLASS_NAMES, 'Final Model (GAN-Balanced) Confusion Matrix')\n",
        "\n",
        "print(\"\\nEvaluation complete. Compare the two classification reports.\")"
      ]
    }
  ]
}